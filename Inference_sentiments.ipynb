{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Forecasting with Reddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Setup](#toc1_)    \n",
    "2. [Model](#toc2_)    \n",
    "3. [Functions](#toc3_)    \n",
    "3.1. [Prepare Data](#toc3_1_)    \n",
    "3.2. [Train & Evaluate](#toc3_2_)    \n",
    "3.3. [Sentiment Scores](#toc3_3_)    \n",
    "3.4. [Load Data](#toc3_4_)    \n",
    "3.5. [Predict](#toc3_5_)    \n",
    "4. [Hyperparameter tuning](#toc4_)    \n",
    "5. [Forecast](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\sohmt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sohmt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sohmt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from pprint import pprint\n",
    "from prettytable import PrettyTable\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "from itertools import product\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from pprint import pprint\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "import datetime\n",
    "import shap\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as pdr\n",
    "import yfinance as yf\n",
    "import praw\n",
    "import prawcore.exceptions\n",
    "\n",
    "import datetime\n",
    "import datetime as dt\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers, output_size):\n",
    "        super(DynamicLSTM, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.LSTM(input_size, hidden_sizes[0], num_layers=1, batch_first=True))\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.hidden_layers.append(nn.LSTM(hidden_sizes[i-1], hidden_sizes[i], num_layers=1, batch_first=True))\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x, _ = layer(x.to(device))  # Move input to GPU\n",
    "        x = self.output_layer(x[:, -1, :])  # Take the output from the last time step\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[Prepare Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data_multivariate(df, choosen_stock, startdate, enddate, features, look_back, predict_type='year'):\n",
    "    # Choose specific stock\n",
    "    data = df[df[\"Stock\"] == choosen_stock]\n",
    "\n",
    "    # Test split\n",
    "    if predict_type=='year':\n",
    "        test_data = data[data[\"Date\"].dt.year == 2023]\n",
    "    elif predict_type=='month':\n",
    "        test_data = data[(data[\"Date\"].dt.year == 2023) & (data[\"Date\"].dt.month.isin([1]))]\n",
    "    elif predict_type=='days':\n",
    "        test_data = data[data[\"Date\"].dt.year == 2023][0:20] \n",
    "    elif predict_type=='forecast':\n",
    "        test_data = data[(data[\"Date\"] >= dt.datetime(2023, 10, 12)) & (data[\"Date\"] <= enddate)]\n",
    "\n",
    "    # Train split\n",
    "    if predict_type=='forecast':\n",
    "        train_data = data[(data[\"Date\"] >= startdate) & (data[\"Date\"] <= dt.datetime(2023, 10, 11))]\n",
    "    else:\n",
    "        train_data = data[(data[\"Date\"] >= startdate) & (data[\"Date\"] <= dt.datetime(2022, 12, 31))]\n",
    "    \n",
    "    # Feature selection and engineering\n",
    "    train_data = train_data[features + [\"Date\"]].values\n",
    "    test_data = test_data[features + [\"Date\"]].values\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_data[:, :-1] = scaler.fit_transform(train_data[:, :-1])\n",
    "    test_data[:, :-1] = scaler.transform(test_data[:, :-1])\n",
    "    \n",
    "    # Create sequences for LSTM input\n",
    "    def create_sequences(dataset, look_back):\n",
    "        X, Y, dates = [], [], []\n",
    "        for i in range(len(dataset) - look_back):\n",
    "            X.append(dataset[i:(i + look_back), :-1])\n",
    "            Y.append(dataset[i + look_back, 0])\n",
    "            dates.append(dataset[i + look_back, -1])  # Assuming the last column is 'Date'\n",
    "        return np.array(X), np.array(Y), np.array(dates)\n",
    "    train_X, train_Y, train_dates = create_sequences(train_data, look_back)\n",
    "    test_X, test_Y, test_dates = create_sequences(test_data, look_back)\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to GPU\n",
    "    train_X = torch.Tensor(train_X.astype(np.float32)).to(device)\n",
    "    train_Y = torch.Tensor(train_Y).to(device)\n",
    "    test_X = torch.Tensor(test_X.astype(np.float32)).to(device)\n",
    "    test_Y = torch.Tensor(test_Y).to(device)\n",
    "    \n",
    "    return train_X, train_Y, train_dates, test_X, test_Y, test_dates, scaler, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[Train & Evaluate](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_lstm_multivariate(input_size, hidden_sizes, num_layers, output_size, learning_rate, num_epochs, train_X, train_Y, test_X, test_Y, scaler, test_data, test_dates, visualize=True):\n",
    "    # Initialize the model\n",
    "    # model = MultivariateLSTMModel(input_size, hidden_sizes, num_layers, output_size)\n",
    "    # model = DynamicLSTM(input_size, hidden_sizes, num_layers, output_size)\n",
    "    model = DynamicLSTM(input_size, hidden_sizes, num_layers, output_size).to(device)\n",
    "    # print(model)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Move data to GPU\n",
    "    train_X, train_Y, test_X, test_Y = train_X.to(device), train_Y.to(device), test_X.to(device), test_Y.to(device)\n",
    "    print(train_X.shape)\n",
    "\n",
    "    # Training the model\n",
    "    train_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        outputs = model(train_X)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs.view(-1), train_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate predictions\n",
    "    model.eval()\n",
    "    train_predict = model(train_X).view(-1).cpu().detach().numpy()\n",
    "    test_predict = model(test_X).view(-1).cpu().detach().numpy()\n",
    "    \n",
    "    # Compute RMSE & MAPE\n",
    "    train_rmse = mean_squared_error(train_Y.cpu(), train_predict, squared=False)\n",
    "    test_rmse = mean_squared_error(test_Y.cpu(), test_predict, squared=False)\n",
    "    train_mape = mean_absolute_percentage_error(train_Y.cpu(), train_predict)\n",
    "    test_mape = mean_absolute_percentage_error(test_Y.cpu(), test_predict)\n",
    "    \n",
    "    # Inverse Scaling\n",
    "    # --> 1.test_predict\n",
    "    test_data1 = test_data[:, 1:-1]\n",
    "    # Ensure the second array has the same number of rows as the first array\n",
    "    test_data1 = test_data1[:test_predict.reshape(-1, 1).shape[0], :]\n",
    "    # Append the arrays\n",
    "    test_data1 = np.hstack((test_predict.reshape(-1, 1), test_data1)) \n",
    "    test_predict_inverse = scaler.inverse_transform(test_data1)[:,0]\n",
    "    \n",
    "    # --> 2.test_Y\n",
    "    test_data2 = test_data[:, :-1]\n",
    "    test_data2 = test_data2[:test_predict.reshape(-1, 1).shape[0], :]\n",
    "    test_Y_inverse = scaler.inverse_transform(test_data2)[:,0]\n",
    "    \n",
    "    # Visualize test and predictions\n",
    "    if visualize == True:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(test_dates, test_Y_inverse, label='True', linewidth=2)\n",
    "        plt.plot(test_dates, test_predict_inverse, label='Predicted', linewidth=2)\n",
    "        plt.title(\"Test vs. Predicted Prices\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price\")\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    return model, loss, train_rmse, test_rmse, train_mape, test_mape, test_Y_inverse, test_predict_inverse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[Sentiment Scores](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Script to scrape Reddit posts related to all stocks from different subreddits\n",
    "\n",
    "# Initialize the Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"2aV6_rxA1c44CoHtxY6e0A\",\n",
    "    client_secret=\"7rbu00bmyQOlnHCu-xjyTa-U5har-g\",\n",
    "    user_agent=\"Project Capstone\"\n",
    ")\n",
    "\n",
    "# list of subreddits to search\n",
    "subreddits_to_search = ['stocks', 'investing', 'wallstreetbets', 'finance', 'economy', 'stockmarket', 'business']\n",
    "\n",
    "# list of search queries\n",
    "search_queries = ['Google Stocks OR GOOG', 'Microsoft Stocks OR MSFT', 'Amazon Stocks or AMZN', 'Netflix Stocks OR NFLX', 'TESLA Stocks OR TSLA','Nvidia stocks or NVDA', 'GameStop Corp or GME' ]\n",
    "\n",
    "# Define the date range (January 1, 2015, to December 31, 2020) for consistency\n",
    "start_date = datetime.date(2018, 1, 1)\n",
    "end_date = datetime.date(2023, 12, 4)\n",
    "\n",
    "# lists to store data\n",
    "titles = []\n",
    "authors = []\n",
    "scores = []\n",
    "urls = []\n",
    "contents = []\n",
    "post_dates = []\n",
    "subreddit_names = []\n",
    "query_names = []\n",
    "\n",
    "# Iterate through each subreddit\n",
    "for subreddit_name in subreddits_to_search:\n",
    "    for search_query in search_queries:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            posts = subreddit.search(search_query, limit=None)\n",
    "\n",
    "            # Iterate through the search results and collect data\n",
    "            for post in posts:\n",
    "                post_date = pd.to_datetime(post.created_utc, unit='s').date()\n",
    "                if start_date <= post_date <= end_date:\n",
    "                    if post.selftext:  # Check if the post has text content\n",
    "                        titles.append(post.title)\n",
    "                        authors.append(post.author)\n",
    "                        scores.append(post.score)\n",
    "                        urls.append(post.url)\n",
    "                        contents.append(post.selftext)\n",
    "                        post_dates.append(pd.to_datetime(post.created_utc, unit='s'))\n",
    "                        subreddit_names.append(subreddit_name)\n",
    "                        query_names.append(search_query[-4:])\n",
    "\n",
    "        # handle the exceptions\n",
    "        except prawcore.exceptions.NotFound as e:\n",
    "            print(f\"Error in subreddit '{subreddit_name}': {e}\")\n",
    "        except prawcore.exceptions.Forbidden as e:\n",
    "            print(f\"Access forbidden in subreddit '{subreddit_name}': {e}\")\n",
    "        except praw.exceptions.APIException as e:\n",
    "            print(f\"API Error: {e}\")\n",
    "            # Wait for a while before retrying (e.g., 5 seconds)\n",
    "            time.sleep(5)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "data = {\n",
    "    'Stock': query_names,\n",
    "    'Subreddit': subreddit_names,\n",
    "    'Title': titles,\n",
    "    'Author': authors,\n",
    "    'Score': scores,\n",
    "    'URL': urls,\n",
    "    'Content': contents,\n",
    "    'Date': post_dates,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# drop duplicate\n",
    "df = df.drop_duplicates(subset='URL')\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "df_reddit = df\n",
    "\n",
    "# drop the specific column\n",
    "# df_reddit.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "# Combine columns\n",
    "df_reddit['Text'] = df_reddit['Title'] + ' ' + df_reddit['Content']\n",
    "\n",
    "# Select specific columns to consider\n",
    "selected_columns = ['Date','Text','Stock']\n",
    "df_reddit = df_reddit[selected_columns]\n",
    "\n",
    "# Convert to suitable datatype\n",
    "df_reddit.loc[:, 'Date'] = pd.to_datetime(df_reddit['Date']).dt.date\n",
    "\n",
    "# Define a function to clean and preprocess text\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Rejoin the tokens to form cleaned text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the clean_text function to 'Title' and 'Content' columns using lambda\n",
    "df_reddit.loc[:, 'Cleaned_Text'] = df_reddit['Text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       Unnamed: 0        Date  \\\n",
      "0           2448  2018-01-01   \n",
      "1           2052  2018-01-02   \n",
      "2           2535  2018-01-05   \n",
      "3           3447  2018-01-08   \n",
      "4            152  2018-01-13   \n",
      "...          ...         ...   \n",
      "4589         801  2023-12-01   \n",
      "4590        1917  2023-12-01   \n",
      "4591        1939  2023-12-02   \n",
      "4592        2225  2023-12-03   \n",
      "4593         645  2023-12-04   \n",
      "\n",
      "                                                   Text Stock  \\\n",
      "0     My Stock Estimations for 2020 Today is 1-1-201...  NVDA   \n",
      "1     Citibank : 40% probability of Apple buying Net...  NFLX   \n",
      "2     GME The bulls say it’s a value play.  The bear...   GME   \n",
      "3     NVDA SPACE BASE TO NEW ALL TIME HIGHS WE GO\\n\\...  NVDA   \n",
      "4     AMZN or GOOG (GOOGL) for long term hold? I can...  GOOG   \n",
      "...                                                 ...   ...   \n",
      "4589  Apple TV Plus and Paramount Plus reportedly di...  NFLX   \n",
      "4590  How do I pivot away from dividend investing? I...  AMZN   \n",
      "4591  Need help in explaining about investment and r...  AMZN   \n",
      "4592  What EV stocks are the best buys today? A lot ...  TSLA   \n",
      "4593  What are the fundamental traits of promising c...  AMZN   \n",
      "\n",
      "                                           Cleaned_Text  VADER_sentiment  \n",
      "0     stock estimation 2020 today 112018 call 112020...          -0.8402  \n",
      "1     citibank 40 probability apple buying netflix h...           0.4404  \n",
      "2     gme bull say ’ value play bear say blockbuster...           0.9231  \n",
      "3     nvda space base new time high go httpsblogsnvi...           0.0000  \n",
      "4     amzn goog googl long term hold assume might co...           0.5391  \n",
      "...                                                 ...              ...  \n",
      "4589  apple tv plus paramount plus reportedly discus...           0.9725  \n",
      "4590  pivot away dividend investing widowed twice 3 ...           0.9789  \n",
      "4591  need help explaining investment return edit th...           0.9666  \n",
      "4592  ev stock best buy today lot ev related stock p...           0.9512  \n",
      "4593  fundamental trait promising company looking bu...           0.9934  \n",
      "\n",
      "[4594 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "def perform_sentiment_analysis(df):\n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Perform sentiment analysis for each 'text' in the DataFrame\n",
    "    sentiment_scores = df['Text'].apply(lambda x: sia.polarity_scores(x))\n",
    "\n",
    "    # Map the sentiment scores to sentiment labels\n",
    "    sentiment_labels = sentiment_scores.apply(lambda x: x['compound'] if x['compound'] > 0 else x['compound'] if x['compound'] < 0 else x['compound'])\n",
    "\n",
    "    # Add sentiment labels to the DataFrame\n",
    "    df['VADER_sentiment'] = sentiment_labels\n",
    "\n",
    "    return df\n",
    "\n",
    "df_reddit = perform_sentiment_analysis(df_reddit)\n",
    "\n",
    "\n",
    "df_reddit.to_csv('RedditPosts.csv')\n",
    "\n",
    "df_reddit = pd.read_csv(\"RedditPosts.csv\")\n",
    "print(df_reddit.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[Load Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(choosen_stock, predict_type):    \n",
    "    yf.pdr_override() # Override pandas datareader with yfinance\n",
    "    y_symbols = [choosen_stock]\n",
    "\n",
    "    # Train split\n",
    "    if predict_type=='forecast':\n",
    "        # State the dates\n",
    "        startdate = dt.datetime(2018, 1, 1) # start date\n",
    "        enddate = dt.datetime(2023, 12, 4) # end date\n",
    "\n",
    "        # Retrieve historical stock price data for the specified symbols and date range\n",
    "        df = yf.download(y_symbols, start=startdate, end=enddate) \n",
    "        df = df.reset_index() # Reset the index to make 'Date' a regular column\n",
    "        df['Stock'] = choosen_stock # add 'Stock' column\n",
    "        df = df[['Date', 'Stock', 'Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']] # Reorder the columns\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # add new row\n",
    "        new_row = [{'Date':pd.to_datetime('2023-12-4T00:00:00.000000000'), 'Stock': choosen_stock , 'Adj Close': 0.0, 'Close': 0.0 , 'High': 0.0,'Low': 0.0, 'Open':0.0, 'Volume':0.0}]\n",
    "        df = pd.concat([df, pd.DataFrame(new_row)], ignore_index=True)\n",
    "        new_row = [{'Date':pd.to_datetime('2023-12-5T00:00:00.000000000'), 'Stock': choosen_stock , 'Adj Close': 0.0, 'Close': 0.0 , 'High': 0.0,'Low': 0.0, 'Open':0.0, 'Volume':0.0}]\n",
    "        df = pd.concat([df, pd.DataFrame(new_row)], ignore_index=True)\n",
    "        new_row = [{'Date':pd.to_datetime('2023-12-6T00:00:00.000000000'), 'Stock': choosen_stock , 'Adj Close': 0.0, 'Close': 0.0 , 'High': 0.0,'Low': 0.0, 'Open':0.0, 'Volume':0.0}]\n",
    "        df = pd.concat([df, pd.DataFrame(new_row)], ignore_index=True)\n",
    "        new_row = [{'Date':pd.to_datetime('2023-12-7T00:00:00.000000000'), 'Stock': choosen_stock , 'Adj Close': 0.0, 'Close': 0.0 , 'High': 0.0,'Low': 0.0, 'Open':0.0, 'Volume':0.0}]\n",
    "        df = pd.concat([df, pd.DataFrame(new_row)], ignore_index=True)\n",
    "        new_row = [{'Date':pd.to_datetime('2023-12-8T00:00:00.000000000'), 'Stock': choosen_stock , 'Adj Close': 0.0, 'Close': 0.0 , 'High': 0.0,'Low': 0.0, 'Open':0.0, 'Volume':0.0}]\n",
    "        df = pd.concat([df, pd.DataFrame(new_row)], ignore_index=True)\n",
    "        \n",
    "        return df, startdate, dt.datetime(2023, 12, 8) \n",
    "    \n",
    "    else:\n",
    "        startdate = dt.datetime(2015, 1, 1) # start date\n",
    "        enddate = dt.datetime(2023, 11, 1) # end date\n",
    "        \n",
    "        # Retrieve historical stock price data for the specified symbols and date range\n",
    "        df = yf.download(y_symbols, start=startdate, end=enddate) \n",
    "        df = df.reset_index() # Reset the index to make 'Date' a regular column\n",
    "        df['Stock'] = choosen_stock # add 'Stock' column\n",
    "        df = df[['Date', 'Stock', 'Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']] # Reorder the columns\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "  \n",
    "        return df, startdate, enddate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. <a id='toc3_5_'></a>[Predict](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standalone prediction\n",
    "def predict(choosen_stock):\n",
    "    # Load the data\n",
    "    data, startdate, enddate = load_data(choosen_stock)\n",
    "\n",
    "    # Append the last row to the DataFrame\n",
    "    new_row = [{'Date':pd.to_datetime('2023-12-01T00:00:00.000000000'), 'Stock': choosen_stock , 'Adj Close': 0.0, 'Close': 0.0 , 'High': 0.0,'Low': 0.0, 'Open':0.0, 'Volume':0.0}]\n",
    "    data = pd.concat([data, pd.DataFrame(new_row)], ignore_index=True)\n",
    "\n",
    "    # 1. Hyperparameters\n",
    "    look_back = 5 # No. of Lags to consider\n",
    "    predict_type = 'Predict' # Predict type ['Year', 'Month', 'Days','Predict']\n",
    "    features = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
    "    predict_type = 'Predict'                            # Predict type ['Year', 'Month', 'Days','Predict']\n",
    "    hidden_sizes = [64, 64]                   # Adjust the hidden_size values as needed\n",
    "    num_layers = 2                       # [1, 2, 3]\n",
    "    learning_rate = 0.001         # [0.005, 0.01, 0.02]  \n",
    "    num_epochs = 100                                    # [50, 100, 200] \n",
    "\n",
    "    # Prepare the data\n",
    "    train_X, train_Y, train_dates, test_X, test_Y, test_dates, scaler, test_data = prepare_data_multivariate(data, choosen_stock, startdate, enddate, features=features, look_back=look_back, predict_type=predict_type )\n",
    "\n",
    "    # 2. Hyperparameters\n",
    "    input_size = 5  # Number of input features (High, Low, Open, Close, Volume)\n",
    "    output_size = 1  # Number of output features (Close price)\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Create the model\n",
    "    model, loss, train_rmse, test_rmse, train_mape, test_mape, test_Y_inverse, test_predict_inverse  = train_evaluate_lstm_multivariate(input_size, hidden_sizes, num_layers, output_size, learning_rate, num_epochs, train_X, train_Y, test_X, test_Y, scaler, test_data, visualize=False)\n",
    "    \n",
    "    # Formatting the prices to a desired decimal form\n",
    "    formatted_test_Y = [\"{:.4f}\".format(price) for price in test_Y_inverse.flatten()]\n",
    "    formatted_test_predict = [\"{:.4f}\".format(price) for price in test_predict_inverse.flatten()]\n",
    "    formatted_dates = [test_dates.strftime('%Y-%m-%d') for test_dates in test_dates]\n",
    "\n",
    "    return formatted_test_Y, formatted_test_predict, formatted_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[Hyperparameter tuning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOG\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Number of NaN values in 'new_column': 1106\n",
      "Number of NaN values in 'new_column': 26\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "torch.Size([1423, 5, 6])\n",
      "\n",
      "MSFT\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Number of NaN values in 'new_column': 1112\n",
      "Number of NaN values in 'new_column': 20\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "torch.Size([1429, 5, 6])\n",
      "\n",
      "AMZN\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Number of NaN values in 'new_column': 1071\n",
      "Number of NaN values in 'new_column': 14\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "torch.Size([1435, 5, 6])\n",
      "\n",
      "NFLX\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Number of NaN values in 'new_column': 1192\n",
      "Number of NaN values in 'new_column': 0\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "torch.Size([1449, 5, 6])\n",
      "\n",
      "TSLA\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Number of NaN values in 'new_column': 1153\n",
      "Number of NaN values in 'new_column': 48\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "torch.Size([1401, 5, 6])\n",
      "\n",
      "NVDA\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Number of NaN values in 'new_column': 1111\n",
      "Number of NaN values in 'new_column': 4\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "torch.Size([1445, 5, 6])\n",
      "Top Performing Models:\n",
      "+--------+-------+-----------+---------------+------------+-------------+------------+-------------+-----------+-----------+\n",
      "| Ticker | Model | Look Back | Learning Rate | Input Size | Hidden Size | Num Layers | Output Size | Test RMSE | Test MAPE |\n",
      "+--------+-------+-----------+---------------+------------+-------------+------------+-------------+-----------+-----------+\n",
      "|  GOOG  |   1   |     5     |      0.01     |     6      |    (128,)   |     1      |      1      |   0.2763  |   0.2047  |\n",
      "|  MSFT  |   1   |     5     |      0.01     |     6      |    (64,)    |     1      |      1      |   0.276   |   0.3015  |\n",
      "|  AMZN  |   1   |     5     |      0.01     |     6      |    (64,)    |     1      |      1      |   0.2584  |   0.1794  |\n",
      "|  NFLX  |   1   |     5     |      0.01     |     6      |    (128,)   |     1      |      1      |   0.1866  |   0.2107  |\n",
      "|  TSLA  |   1   |     5     |      0.01     |     6      |    (128,)   |     1      |      1      |   0.1224  |   1.1782  |\n",
      "|  NVDA  |   1   |     5     |      0.01     |     6      |    (128,)   |     1      |      1      |   0.2041  |   0.8844  |\n",
      "+--------+-------+-----------+---------------+------------+-------------+------------+-------------+-----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the best models for each ticker\n",
    "all_best_models = {}\n",
    "\n",
    "for ticker in ['GOOG', 'MSFT', 'AMZN', 'NFLX', 'TSLA', 'NVDA']:#, 'GME']:\n",
    "    print(\"\\n\" + ticker)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    choosen_stock = ticker                              # ['GOOG', 'MSFT', 'META', 'AMZN', 'NFLX', 'TSLA', 'NVDA']\n",
    "    look_back_values = [5]        \n",
    "    # No. of Lags to consider [3, 5, 10, 20]\n",
    "    predict_type = 'forecast'                            # Predict type ['year', 'month', 'days','predict']\n",
    "    hidden_size_options = [64, 128]                      # Adjust the hidden_size values as needed\n",
    "    num_layers_values = [1,2]                       # [1, 2, 3]\n",
    "    learning_rate_values = [0.001, 0.01, 0.005]         # [0.005, 0.01, 0.02]  \n",
    "    num_epochs = 100                                    # [50, 100, 200] \n",
    "    top_k = 1                                           # Get the top k performing models\n",
    "\n",
    "    # ?\n",
    "    years_to_include = [2015, 2016, 2017, 2018, 2019]\n",
    "    train_days_values = [-1, 60, 100, 365, 365*2]\n",
    "\n",
    "\n",
    "    # Features\n",
    "    features = ['Close', 'High', 'Low', 'Open', 'Volume', 'reddit_compound']\n",
    "    input_size = 6  # Number of input features (High, Low, Open, Close, Volume)\n",
    "    output_size = 1  # Number of output features (Close price)\n",
    "\n",
    "\n",
    "    # Load the data\n",
    "    df, startdate, enddate = load_data(choosen_stock, predict_type)\n",
    "    \n",
    "    # print(df.head)\n",
    "    # print(df.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_reddit = pd.read_csv(\"RedditPosts.csv\")\n",
    "    df_reddit = df_reddit[df_reddit[\"Stock\"] == choosen_stock]\n",
    "    df_reddit.rename(columns = {'VADER_sentiment':'reddit_compound'}, inplace = True)\n",
    "    sentiments_reddit = df_reddit.groupby('Date')['reddit_compound'].mean()\n",
    "    df_reddit = pd.DataFrame(sentiments_reddit)\n",
    "    df_reddit = df_reddit.reset_index()\n",
    "    df_reddit['Date'] = pd.to_datetime(df_reddit['Date'])\n",
    "    \n",
    "    \n",
    "    df = df.merge(df_reddit, left_on = 'Date', right_on='Date',  how='left')\n",
    "    \n",
    "    # print(df.head)\n",
    "    # print(df.shape)\n",
    "\n",
    "    # df['all_compound'] = np.select(conditions, choices, default=np.nan)\n",
    "    nan_count = df['reddit_compound'].isnull().sum()\n",
    "    print(\"Number of NaN values in 'new_column':\", nan_count)\n",
    "    df['reddit_compound'] = df['reddit_compound'].ffill()\n",
    "    filtered_df = df[df['Stock']==choosen_stock]\n",
    "    nan_count = filtered_df['reddit_compound'].isna().sum()\n",
    "    print(\"Number of NaN values in 'new_column':\", nan_count)\n",
    "    filtered_df.head()\n",
    "    \n",
    "    df = filtered_df\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Store best_models\n",
    "    best_models = []\n",
    "\n",
    "    # Hyperparameter tuning loop\n",
    "\n",
    "    for num_layers in num_layers_values:\n",
    "        hidden_size_values = list(product(hidden_size_options, repeat=num_layers))\n",
    "        # print(hidden_size_values)\n",
    "        for hidden_size in hidden_size_values:\n",
    "            # print(hidden_size)\n",
    "            for look_back in look_back_values:\n",
    "                for learning_rate in learning_rate_values:\n",
    "                    # print(f\"\\nHyperparameters: look_back={look_back}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}\")\n",
    "                    # Prepare the data\n",
    "                    train_X, train_Y, train_dates, test_X, test_Y, test_dates, scaler, test_data = prepare_data_multivariate(df, choosen_stock, startdate, enddate, features=features, look_back=look_back, predict_type=predict_type)\n",
    "                    model, loss, train_rmse, test_rmse, train_mape, test_mape, test_Y_inverse, test_predict_inverse = train_evaluate_lstm_multivariate(input_size, hidden_size, num_layers, output_size, learning_rate, num_epochs, train_X, train_Y, test_X, test_Y, scaler, test_data, test_dates, visualize=False)\n",
    "                    best_models.append({\n",
    "                        \"model\" : model,\n",
    "                        \"look_back\" : look_back,\n",
    "                        \"learning_rate\" : learning_rate,\n",
    "                        \"input_size\" : input_size,\n",
    "                        \"hidden_size\" : hidden_size,\n",
    "                        \"num_layers\" : num_layers,\n",
    "                        \"output_size\" : output_size,\n",
    "                        \"test_rmse\": test_rmse,\n",
    "                        \"test_mape\": test_mape,\n",
    "                        \"test_X\": test_X,\n",
    "                        \"test_Y_inverse\":test_Y_inverse,\n",
    "                        \"test_predict_inverse\":test_predict_inverse,\n",
    "                        \"test_dates\":test_dates,\n",
    "                        \"test_data\" : test_data,\n",
    "                    })\n",
    "\n",
    "    # Sort the models by RMSE in ascending order\n",
    "    best_models.sort(key=lambda x: (x[\"test_rmse\"], x[\"test_mape\"]))\n",
    "\n",
    "    # Store the top-k performing models for the current ticker\n",
    "    all_best_models[ticker] = best_models[:top_k]\n",
    "\n",
    "# Save Parameters & Weights\n",
    "for ticker, models in all_best_models.items():\n",
    "    for idx, model_info in enumerate(models, start=1):\n",
    "        # Save parameters for the model reload\n",
    "        params_file_name = \"Saved Params/\" + ticker + \"_params.pkl\"\n",
    "        with open(params_file_name, 'wb') as file:\n",
    "            pickle.dump(model_info, file)\n",
    "        # print(f\"Model Parameters for {ticker} saved to {params_file_name}\")\n",
    "        \n",
    "        # Save the model weights to a pickle file\n",
    "        model_filename = f\"Saved Models/{ticker}_model\"\n",
    "        torch.save(model_info[\"model\"].state_dict(), model_filename)\n",
    "        # print(f\"Model weights for {ticker} saved to {model_filename}\")\n",
    "\n",
    "\n",
    "# Print the best models for each ticker\n",
    "# for ticker, models in all_best_models.items():\n",
    "#     print(f\"\\nTop {top_k} Performing Model for {ticker}:\")\n",
    "#     pprint(models)\n",
    "\n",
    "# Assuming all_best_models is a dictionary with tickers as keys and a list of best models as values\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Ticker\", \"Model\", \"Look Back\", \"Learning Rate\", \"Input Size\", \"Hidden Size\", \"Num Layers\", \"Output Size\", \"Test RMSE\", \"Test MAPE\"]\n",
    "\n",
    "for ticker, models in all_best_models.items():\n",
    "    for idx, model_info in enumerate(models, start=1):\n",
    "        table.add_row([\n",
    "            ticker,\n",
    "            idx,\n",
    "            model_info[\"look_back\"],\n",
    "            model_info[\"learning_rate\"],\n",
    "            model_info[\"input_size\"],\n",
    "            model_info[\"hidden_size\"],\n",
    "            model_info[\"num_layers\"],\n",
    "            model_info[\"output_size\"],\n",
    "            round(model_info[\"test_rmse\"], 4),\n",
    "            round(model_info[\"test_mape\"], 4),\n",
    "        ])\n",
    "\n",
    "# Print the combined table\n",
    "print(\"Top Performing Models:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[Forecast](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOG\n",
      "+------------+--------------+------------------+\n",
      "|    Date    | Actual Price | Predicted Price  |\n",
      "+------------+--------------+------------------+\n",
      "| 2023-11-20 |    137.92    |      136.88      |\n",
      "| 2023-11-21 |    138.62    |      137.07      |\n",
      "| 2023-11-22 |    140.02    |      137.79      |\n",
      "| 2023-11-24 |    138.22    |      138.71      |\n",
      "| 2023-11-27 |    138.05    |      138.9       |\n",
      "| 2023-11-28 |    138.62    |      138.69      |\n",
      "| 2023-11-29 |    136.4     |      138.58      |\n",
      "| 2023-11-30 |    133.92    |      138.27      |\n",
      "| 2023-12-01 |    133.32    |      136.56      |\n",
      "| ---------- | ------------ | ---------------- |\n",
      "| 2023-12-04 |     TBA      |      135.28      |\n",
      "+------------+--------------+------------------+\n",
      "\n",
      "MSFT\n",
      "+------------+--------------+------------------+\n",
      "|    Date    | Actual Price | Predicted Price  |\n",
      "+------------+--------------+------------------+\n",
      "| 2023-11-20 |    377.44    |      360.47      |\n",
      "| 2023-11-21 |    373.07    |      360.98      |\n",
      "| 2023-11-22 |    377.85    |      361.81      |\n",
      "| 2023-11-24 |    377.43    |      363.45      |\n",
      "| 2023-11-27 |    378.61    |      364.03      |\n",
      "| 2023-11-28 |    382.7     |      364.74      |\n",
      "| 2023-11-29 |    378.85    |      366.84      |\n",
      "| 2023-11-30 |    378.91    |      366.14      |\n",
      "| 2023-12-01 |    374.51    |      365.91      |\n",
      "| ---------- | ------------ | ---------------- |\n",
      "| 2023-12-04 |     TBA      |      364.74      |\n",
      "+------------+--------------+------------------+\n",
      "\n",
      "AMZN\n",
      "+------------+--------------+------------------+\n",
      "|    Date    | Actual Price | Predicted Price  |\n",
      "+------------+--------------+------------------+\n",
      "| 2023-11-20 |    146.13    |      143.63      |\n",
      "| 2023-11-21 |    143.9     |      144.49      |\n",
      "| 2023-11-22 |    146.71    |      143.68      |\n",
      "| 2023-11-24 |    146.74    |      145.15      |\n",
      "| 2023-11-27 |    147.73    |      145.84      |\n",
      "| 2023-11-28 |    147.03    |      146.76      |\n",
      "| 2023-11-29 |    146.32    |      146.96      |\n",
      "| 2023-11-30 |    146.09    |      147.34      |\n",
      "| 2023-12-01 |    147.03    |      146.82      |\n",
      "| ---------- | ------------ | ---------------- |\n",
      "| 2023-12-04 |     TBA      |      147.15      |\n",
      "+------------+--------------+------------------+\n",
      "\n",
      "NFLX\n",
      "+------------+--------------+------------------+\n",
      "|    Date    | Actual Price | Predicted Price  |\n",
      "+------------+--------------+------------------+\n",
      "| 2023-11-20 |    474.47    |      465.18      |\n",
      "| 2023-11-21 |    474.95    |      469.72      |\n",
      "| 2023-11-22 |    478.0     |      473.89      |\n",
      "| 2023-11-24 |    479.56    |      473.31      |\n",
      "| 2023-11-27 |    479.17    |      475.48      |\n",
      "| 2023-11-28 |    479.0     |      476.41      |\n",
      "| 2023-11-29 |    477.19    |      477.04      |\n",
      "| 2023-11-30 |    473.97    |      477.33      |\n",
      "| 2023-12-01 |    465.74    |      475.19      |\n",
      "| ---------- | ------------ | ---------------- |\n",
      "| 2023-12-04 |     TBA      |      476.56      |\n",
      "+------------+--------------+------------------+\n",
      "\n",
      "TSLA\n",
      "+------------+--------------+------------------+\n",
      "|    Date    | Actual Price | Predicted Price  |\n",
      "+------------+--------------+------------------+\n",
      "| 2023-11-20 |    235.6     |      235.47      |\n",
      "| 2023-11-21 |    241.2     |      236.53      |\n",
      "| 2023-11-22 |    234.21    |      238.06      |\n",
      "| 2023-11-24 |    235.45    |      238.61      |\n",
      "| 2023-11-27 |    236.08    |      238.51      |\n",
      "| 2023-11-28 |    246.72    |      237.47      |\n",
      "| 2023-11-29 |    244.14    |      238.87      |\n",
      "| 2023-11-30 |    240.08    |      243.01      |\n",
      "| 2023-12-01 |    238.83    |      242.8       |\n",
      "| ---------- | ------------ | ---------------- |\n",
      "| 2023-12-04 |     TBA      |      240.53      |\n",
      "+------------+--------------+------------------+\n",
      "\n",
      "NVDA\n",
      "+------------+--------------+------------------+\n",
      "|    Date    | Actual Price | Predicted Price  |\n",
      "+------------+--------------+------------------+\n",
      "| 2023-11-20 |    504.09    |      486.84      |\n",
      "| 2023-11-21 |    499.44    |      489.51      |\n",
      "| 2023-11-22 |    487.16    |      490.8       |\n",
      "| 2023-11-24 |    477.76    |      488.33      |\n",
      "| 2023-11-27 |    482.42    |      483.55      |\n",
      "| 2023-11-28 |    478.21    |      480.54      |\n",
      "| 2023-11-29 |    481.4     |      477.92      |\n",
      "| 2023-11-30 |    467.7     |      477.95      |\n",
      "| 2023-12-01 |    467.65    |      473.73      |\n",
      "| ---------- | ------------ | ---------------- |\n",
      "| 2023-12-04 |     TBA      |      469.47      |\n",
      "+------------+--------------+------------------+\n",
      "\n",
      "GME\n",
      "+------------+--------------+------------------+\n",
      "|    Date    | Actual Price | Predicted Price  |\n",
      "+------------+--------------+------------------+\n",
      "| 2023-11-20 |     12.8     |      12.94       |\n",
      "| 2023-11-21 |    12.55     |       13.0       |\n",
      "| 2023-11-22 |    12.29     |       12.8       |\n",
      "| 2023-11-24 |     12.2     |      12.62       |\n",
      "| 2023-11-27 |    11.91     |      12.45       |\n",
      "| 2023-11-28 |    13.49     |      12.28       |\n",
      "| 2023-11-29 |    16.25     |      12.66       |\n",
      "| 2023-11-30 |    14.55     |      15.26       |\n",
      "| 2023-12-01 |     15.3     |       15.4       |\n",
      "| ---------- | ------------ | ---------------- |\n",
      "| 2023-12-04 |     TBA      |      15.41       |\n",
      "+------------+--------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Forecast next day\n",
    "\n",
    "stocks = ['GOOG', 'MSFT', 'AMZN', 'NFLX', 'TSLA', 'NVDA', 'GME']\n",
    "\n",
    "for choosen_stock in stocks:\n",
    "    print(\"\\n\"+choosen_stock)\n",
    "    \n",
    "    # Function Call\n",
    "    file = open(\"Saved Params/\"+choosen_stock+\"_params.pkl\",'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    model = DynamicLSTM(object_file['input_size'], object_file['hidden_size'], object_file['num_layers'], object_file['output_size']).to(device)\n",
    "    model.load_state_dict(torch.load(\"Saved Models/\"+choosen_stock+\"_model\"))\n",
    "    model.eval()\n",
    "    \n",
    "    test_predict = model(object_file['test_X']).view(-1).cpu().detach().numpy()\n",
    "    test_data = object_file['test_data']\n",
    "    test_predict_inverse = object_file['test_predict_inverse']\n",
    "    test_Y_inverse = object_file['test_Y_inverse']\n",
    "    \n",
    "    formatted_dates = [test_dates.strftime('%Y-%m-%d') for test_dates in object_file[\"test_dates\"]]\n",
    "    formatted_test_Y = [\"{:.4f}\".format(price) for price in test_Y_inverse.flatten()]\n",
    "    formatted_test_predict = [\"{:.4f}\".format(price) for price in test_predict_inverse.flatten()]\n",
    "    \n",
    "    formatted_dates = formatted_dates[-14:-4]\n",
    "    formatted_test_predict = formatted_test_predict[-14:-4]\n",
    "    \n",
    "    # Display\n",
    "    # Remove the first value and shift up the remaining values\n",
    "    actual_prices = formatted_test_Y[0:] + ['']\n",
    "    actual_prices = actual_prices[-10:]\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Date\", \"Actual Price\", \"Predicted Price\"]\n",
    "    for date, actual_price, predicted_price in zip(formatted_dates, actual_prices, formatted_test_predict):\n",
    "        # Add a separator line before the last row\n",
    "        if date == formatted_dates[-1]:\n",
    "            table.add_row([\"-\" * 10, \"-\" * 12, \"-\" * 16])\n",
    "        table.add_row([date, round(float(actual_price), 2) if actual_price != '' else 'TBA', round(float(predicted_price), 2)])\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
