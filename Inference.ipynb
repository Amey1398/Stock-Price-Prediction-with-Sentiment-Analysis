{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Forecasting without Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "1. [Setup](#toc1_)    \n",
    "2. [Model](#toc2_)    \n",
    "3. [Functions](#toc3_)    \n",
    "3.1. [Prepare Data](#toc3_1_)    \n",
    "3.2. [Train & Evaluate](#toc3_2_)    \n",
    "3.3. [Load Data](#toc3_3_)    \n",
    "3.4. [Predict](#toc3_4_)    \n",
    "3.5. [Get Combinations](#toc3_5_)    \n",
    "4. [Hyperparameter tuning](#toc4_)    \n",
    "5. [User Call](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from pprint import pprint\n",
    "from prettytable import PrettyTable\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "from itertools import combinations\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(MultivariateLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_layers, output_size):\n",
    "        super(DynamicLSTM, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.LSTM(input_size, hidden_sizes[0], num_layers=1, batch_first=True))\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.hidden_layers.append(nn.LSTM(hidden_sizes[i-1], hidden_sizes[i], num_layers=1, batch_first=True))\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x, _ = layer(x)\n",
    "        x = self.output_layer(x[:, -1, :])  # Take the output from the last time step\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[Prepare Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_multivariate(df, choosen_stock, startdate, enddate, features, look_back, predict_type='year'):\n",
    "    # Choose specific stock\n",
    "    data = df[df[\"Stock\"] == choosen_stock]\n",
    "\n",
    "    # Test split\n",
    "    if predict_type=='year':\n",
    "        test_data = data[data[\"Date\"].dt.year == 2019]\n",
    "    elif predict_type=='month':\n",
    "        test_data = data[(data[\"Date\"].dt.year == 2019) & (data[\"Date\"].dt.month.isin([1]))]\n",
    "    elif predict_type=='days':\n",
    "        test_data = data[data[\"Date\"].dt.year == 2019][0:20] \n",
    "    else: # Specific\n",
    "        test_data = data[(data[\"Date\"] >= dt.datetime(2023, 11, 12)) & (data[\"Date\"] <= enddate)]\n",
    "        \n",
    "    # Train split\n",
    "    train_data = data[(data[\"Date\"] >= startdate) & (data[\"Date\"] <= dt.datetime(2023, 11, 11))]\n",
    "    \n",
    "    # Feature selection and engineering\n",
    "    train_data = train_data[features + [\"Date\"]].values\n",
    "    test_data = test_data[features + [\"Date\"]].values\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_data[:, :-1] = scaler.fit_transform(train_data[:, :-1])\n",
    "    test_data[:, :-1] = scaler.transform(test_data[:, :-1])\n",
    "    \n",
    "    # Create sequences for LSTM input\n",
    "    def create_sequences(dataset, look_back=1):\n",
    "        X, Y, dates = [], [], []\n",
    "        for i in range(len(dataset) - look_back):\n",
    "            X.append(dataset[i:(i + look_back), :-1])\n",
    "            Y.append(dataset[i + look_back, 0])\n",
    "            dates.append(dataset[i + look_back, -1])  # Assuming the last column is 'Date'\n",
    "        return np.array(X), np.array(Y), np.array(dates)\n",
    "    train_X, train_Y, train_dates = create_sequences(train_data, look_back)\n",
    "    test_X, test_Y, test_dates = create_sequences(test_data, look_back)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    train_X = torch.Tensor(train_X.astype(np.float32))\n",
    "    train_Y = torch.Tensor(train_Y)\n",
    "    test_X = torch.Tensor(test_X.astype(np.float32))\n",
    "    test_Y = torch.Tensor(test_Y)\n",
    "    \n",
    "    return train_X, train_Y, train_dates, test_X, test_Y, test_dates, scaler, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. <a id='toc3_2_'></a>[Train & Evaluate](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_lstm_multivariate(input_size, hidden_sizes, num_layers, output_size, learning_rate, num_epochs, train_X, train_Y, test_X, test_Y, scaler, test_data, visualize=True):\n",
    "    # Initialize the model\n",
    "    # model = MultivariateLSTMModel(input_size, hidden_sizes, num_layers, output_size)\n",
    "    model = DynamicLSTM(input_size, hidden_sizes, num_layers, output_size)\n",
    "    # print(model)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    # Training the model\n",
    "    train_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        outputs = model(train_X)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs.view(-1), train_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate predictions\n",
    "    model.eval()\n",
    "    train_predict = model(train_X).view(-1).cpu().detach().numpy()\n",
    "    test_predict = model(test_X).view(-1).cpu().detach().numpy()\n",
    "    \n",
    "    # Compute RMSE & MAPE\n",
    "    train_rmse = mean_squared_error(train_Y, train_predict, squared=False)\n",
    "    test_rmse = mean_squared_error(test_Y, test_predict, squared=False)\n",
    "    train_mape = mean_absolute_percentage_error(train_Y, train_predict)\n",
    "    test_mape = mean_absolute_percentage_error(test_Y, test_predict)\n",
    "    \n",
    "    # Inverse Scaling\n",
    "    # --> 1.test_predict\n",
    "    test_data1 = test_data[:, 1:-1]\n",
    "    # Ensure the second array has the same number of rows as the first array\n",
    "    test_data1 = test_data1[:test_predict.reshape(-1, 1).shape[0], :]\n",
    "    # Append the arrays\n",
    "    test_data1 = np.hstack((test_predict.reshape(-1, 1), test_data1)) \n",
    "    test_predict_inverse = scaler.inverse_transform(test_data1)[:,0]\n",
    "    \n",
    "    # --> 2.test_Y\n",
    "    test_data2 = test_data[:, :-1]\n",
    "    test_data2 = test_data2[:test_predict.reshape(-1, 1).shape[0], :]\n",
    "    test_Y_inverse = scaler.inverse_transform(test_data2)[:,0]\n",
    "    \n",
    "    # Visualize test and predictions\n",
    "    if visualize == True:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(test_dates, test_Y_inverse, label='True', linewidth=2)\n",
    "        plt.plot(test_dates, test_predict_inverse, label='Predicted', linewidth=2)\n",
    "        plt.title(\"Test vs. Predicted Prices\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price\")\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    return model, loss, train_rmse, test_rmse, train_mape, test_mape, test_Y_inverse, test_predict_inverse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. <a id='toc3_3_'></a>[Load Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(choosen_stock):    \n",
    "    yf.pdr_override() # Override pandas datareader with yfinance\n",
    "    y_symbols = [choosen_stock]\n",
    "    \n",
    "    # State the dates\n",
    "    startdate = dt.datetime(2018, 1, 1) # start date\n",
    "    enddate = dt.datetime(2023, 11, 30) # end date # +1\n",
    "    \n",
    "    # Retrieve historical stock price data for the specified symbols and date range\n",
    "    df = yf.download(y_symbols, start=startdate, end=enddate) \n",
    "    df = df.reset_index() # Reset the index to make 'Date' a regular column\n",
    "    df['Stock'] = choosen_stock # add 'Stock' column\n",
    "    df = df[['Date', 'Stock', 'Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']] # Reorder the columns\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    return df, startdate, enddate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. <a id='toc3_4_'></a>[Predict](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(choosen_stock):\n",
    "    # Load the data\n",
    "    data, startdate, enddate = load_data(choosen_stock)\n",
    "\n",
    "    # Append the last row to the DataFrame\n",
    "    new_row = [{'Date':pd.to_datetime('2023-11-27T00:00:00.000000000'), 'Stock': choosen_stock , 'Adj Close': 0.0, 'Close': 0.0 , 'High': 0.0,'Low': 0.0, 'Open':0.0, 'Volume':0.0}]\n",
    "    data = pd.concat([data, pd.DataFrame(new_row)], ignore_index=True)\n",
    "\n",
    "    # 1. Hyperparameters\n",
    "    look_back = 5 # No. of Lags to consider\n",
    "    predict_type = 'Predict' # Predict type ['Year', 'Month', 'Days','Predict']\n",
    "    features = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
    "    predict_type = 'Predict'                            # Predict type ['Year', 'Month', 'Days','Predict']\n",
    "    hidden_sizes = [64, 64]                   # Adjust the hidden_size values as needed\n",
    "    num_layers = 2                       # [1, 2, 3]\n",
    "    learning_rate = 0.001         # [0.005, 0.01, 0.02]  \n",
    "    num_epochs = 100                                    # [50, 100, 200] \n",
    "\n",
    "    # Prepare the data\n",
    "    train_X, train_Y, train_dates, test_X, test_Y, test_dates, scaler, test_data = prepare_data_multivariate(data, choosen_stock, startdate, enddate, features=features, look_back=look_back, predict_type=predict_type )\n",
    "\n",
    "    # 2. Hyperparameters\n",
    "    input_size = 5  # Number of input features (High, Low, Open, Close, Volume)\n",
    "    output_size = 1  # Number of output features (Close price)\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Create the model\n",
    "    model, loss, train_rmse, test_rmse, train_mape, test_mape, test_Y_inverse, test_predict_inverse  = train_evaluate_lstm_multivariate(input_size, hidden_sizes, num_layers, output_size, learning_rate, num_epochs, train_X, train_Y, test_X, test_Y, scaler, test_data, visualize=False)\n",
    "    \n",
    "    # Formatting the prices to a desired decimal form\n",
    "    formatted_test_Y = [\"{:.4f}\".format(price) for price in test_Y_inverse.flatten()]\n",
    "    formatted_test_predict = [\"{:.4f}\".format(price) for price in test_predict_inverse.flatten()]\n",
    "    formatted_dates = [test_dates.strftime('%Y-%m-%d') for test_dates in test_dates]\n",
    "\n",
    "    return formatted_test_Y, formatted_test_predict, formatted_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. <a id='toc3_5_'></a>[Get Combinations](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations(options, num_layers):\n",
    "    combinations_list = []\n",
    "    for r in range(1, num_layers+1):  # To get combinations of 1, 2, ... num_layers\n",
    "        combinations_list.extend(combinations(options * r, r))\n",
    "    combinations_list = [list(comb) for comb in combinations_list]\n",
    "    # Filter combinations based on length\n",
    "    filtered_combinations = [comb for comb in combinations_list if len(comb) == num_layers]\n",
    "    return filtered_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[Hyperparameter tuning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GOOG\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "MSFT\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "META\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "AMZN\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "NFLX\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "TSLA\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "NVDA\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "Top 1 Performing Model for GOOG:\n",
      "[{'hidden_size': [64, 128],\n",
      "  'learning_rate': 0.001,\n",
      "  'look_back': 5,\n",
      "  'model': DynamicLSTM(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): LSTM(5, 64, batch_first=True)\n",
      "    (1): LSTM(64, 128, batch_first=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "),\n",
      "  'num_layers': 2,\n",
      "  'test_mape': 0.0072738146,\n",
      "  'test_rmse': 0.0077783647}]\n",
      "\n",
      "Top 1 Performing Model for MSFT:\n",
      "[{'hidden_size': [128, 128],\n",
      "  'learning_rate': 0.001,\n",
      "  'look_back': 5,\n",
      "  'model': DynamicLSTM(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): LSTM(5, 128, batch_first=True)\n",
      "    (1): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "),\n",
      "  'num_layers': 2,\n",
      "  'test_mape': 0.006343393,\n",
      "  'test_rmse': 0.007833434}]\n",
      "\n",
      "Top 1 Performing Model for META:\n",
      "[{'hidden_size': [128, 128],\n",
      "  'learning_rate': 0.001,\n",
      "  'look_back': 5,\n",
      "  'model': DynamicLSTM(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): LSTM(5, 128, batch_first=True)\n",
      "    (1): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "),\n",
      "  'num_layers': 2,\n",
      "  'test_mape': 0.0071720914,\n",
      "  'test_rmse': 0.0077323844}]\n",
      "\n",
      "Top 1 Performing Model for AMZN:\n",
      "[{'hidden_size': [64],\n",
      "  'learning_rate': 0.001,\n",
      "  'look_back': 5,\n",
      "  'model': DynamicLSTM(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): LSTM(5, 64, batch_first=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      "),\n",
      "  'num_layers': 1,\n",
      "  'test_mape': 0.008692056,\n",
      "  'test_rmse': 0.008378781}]\n",
      "\n",
      "Top 1 Performing Model for NFLX:\n",
      "[{'hidden_size': [64, 64],\n",
      "  'learning_rate': 0.001,\n",
      "  'look_back': 5,\n",
      "  'model': DynamicLSTM(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): LSTM(5, 64, batch_first=True)\n",
      "    (1): LSTM(64, 64, batch_first=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      "),\n",
      "  'num_layers': 2,\n",
      "  'test_mape': 0.0064014806,\n",
      "  'test_rmse': 0.008020923}]\n",
      "\n",
      "Top 1 Performing Model for TSLA:\n",
      "[{'hidden_size': [128, 128],\n",
      "  'learning_rate': 0.001,\n",
      "  'look_back': 5,\n",
      "  'model': DynamicLSTM(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): LSTM(5, 128, batch_first=True)\n",
      "    (1): LSTM(128, 128, batch_first=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "),\n",
      "  'num_layers': 2,\n",
      "  'test_mape': 0.0074714213,\n",
      "  'test_rmse': 0.0077235894}]\n",
      "\n",
      "Top 1 Performing Model for NVDA:\n",
      "[{'hidden_size': [64, 128],\n",
      "  'learning_rate': 0.001,\n",
      "  'look_back': 5,\n",
      "  'model': DynamicLSTM(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): LSTM(5, 64, batch_first=True)\n",
      "    (1): LSTM(64, 128, batch_first=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "),\n",
      "  'num_layers': 2,\n",
      "  'test_mape': 0.008250976,\n",
      "  'test_rmse': 0.0078735445}]\n"
     ]
    }
   ],
   "source": [
    "## Tuning\n",
    "\n",
    "# Dictionary to store the best models for each ticker\n",
    "all_best_models = {}\n",
    "\n",
    "for ticker in ['GOOG', 'MSFT', 'META', 'AMZN', 'NFLX', 'TSLA', 'NVDA']:\n",
    "    print(\"\\n\" + ticker)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    choosen_stock = \"GOOG\"                              # ['GOOG', 'MSFT', 'META', 'AMZN', 'NFLX', 'TSLA', 'NVDA']\n",
    "    look_back_values = [5]                              # No. of Lags to consider [3, 5, 10, 20]\n",
    "    predict_type = 'Predict'                            # Predict type ['Year', 'Month', 'Days','Predict']\n",
    "    hidden_size_options = [64, 128]                      # Adjust the hidden_size values as needed\n",
    "    num_layers_values = [1, 2]                       # [1, 2, 3]\n",
    "    learning_rate_values = [0.001, 0.01, 0.005]         # [0.005, 0.01, 0.02]  \n",
    "    num_epochs = 100                                    # [50, 100, 200] \n",
    "    top_k = 1                                           # Get the top k performing models\n",
    "\n",
    "    # ?\n",
    "    years_to_include = [2015, 2016, 2017, 2018, 2019]\n",
    "    train_days_values = [-1, 60, 100, 365, 365*2]\n",
    "\n",
    "\n",
    "    # Features\n",
    "    features=['High', 'Low', 'Open', 'Close', 'Volume']\n",
    "    input_size = 5  # Number of input features (High, Low, Open, Close, Volume)\n",
    "    output_size = 1  # Number of output features (Close price)\n",
    "\n",
    "\n",
    "    # Load the data\n",
    "    df, startdate, enddate = load_data(choosen_stock) \n",
    "\n",
    "    # Store best_models\n",
    "    best_models = []\n",
    "\n",
    "    # Hyperparameter tuning loop\n",
    "\n",
    "    for num_layers in num_layers_values:\n",
    "        hidden_size_values = get_combinations(hidden_size_options, num_layers)\n",
    "        # print(hidden_size_values)\n",
    "        for hidden_size in hidden_size_values:\n",
    "            # print(hidden_size)\n",
    "            for look_back in look_back_values:\n",
    "                for learning_rate in learning_rate_values:\n",
    "                    # print(f\"\\nHyperparameters: look_back={look_back}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}\")\n",
    "                    # Prepare the data\n",
    "                    train_X, train_Y, train_dates, test_X, test_Y, test_dates, scaler, test_data = prepare_data_multivariate(df, choosen_stock, startdate, enddate, features=features, look_back=look_back, predict_type=predict_type)\n",
    "                    model, loss, train_rmse, test_rmse, train_mape, test_mape, test_Y_inverse, test_predict_inverse = train_evaluate_lstm_multivariate(input_size, hidden_size, num_layers, output_size, learning_rate, num_epochs, train_X, train_Y, test_X, test_Y, scaler, test_data, visualize=False)\n",
    "                    best_models.append({\n",
    "                        \"model\" : model,\n",
    "                        \"look_back\": look_back,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"hidden_size\" : hidden_size,\n",
    "                        \"num_layers\" : num_layers,\n",
    "                        \"test_rmse\": test_rmse,\n",
    "                        \"test_mape\": test_mape,\n",
    "                    })\n",
    "\n",
    "    # Sort the models by RMSE in ascending order\n",
    "    best_models.sort(key=lambda x: (x[\"test_rmse\"], x[\"test_mape\"]))\n",
    "\n",
    "    # Store the top-k performing models for the current ticker\n",
    "    all_best_models[ticker] = best_models[:top_k]\n",
    "\n",
    "# Print the best models for each ticker\n",
    "for ticker, models in all_best_models.items():\n",
    "    print(f\"\\nTop {top_k} Performing Model for {ticker}:\")\n",
    "    pprint(models)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Performing Models:\n",
      "+--------+-------+-----------+---------------+-------------+------------+-----------+-----------+\n",
      "| Ticker | Model | Look Back | Learning Rate | Hidden Size | Num Layers | Test RMSE | Test MAPE |\n",
      "+--------+-------+-----------+---------------+-------------+------------+-----------+-----------+\n",
      "|  GOOG  |   1   |     5     |     0.001     |  [64, 128]  |     2      |   0.0078  |   0.0073  |\n",
      "|  MSFT  |   1   |     5     |     0.001     |  [128, 128] |     2      |   0.0078  |   0.0063  |\n",
      "|  META  |   1   |     5     |     0.001     |  [128, 128] |     2      |   0.0077  |   0.0072  |\n",
      "|  AMZN  |   1   |     5     |     0.001     |     [64]    |     1      |   0.0084  |   0.0087  |\n",
      "|  NFLX  |   1   |     5     |     0.001     |   [64, 64]  |     2      |   0.008   |   0.0064  |\n",
      "|  TSLA  |   1   |     5     |     0.001     |  [128, 128] |     2      |   0.0077  |   0.0075  |\n",
      "|  NVDA  |   1   |     5     |     0.001     |  [64, 128]  |     2      |   0.0079  |   0.0083  |\n",
      "+--------+-------+-----------+---------------+-------------+------------+-----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# print the models\n",
    "\n",
    "# Assuming all_best_models is a dictionary with tickers as keys and a list of best models as values\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Ticker\", \"Model\", \"Look Back\", \"Learning Rate\", \"Hidden Size\", \"Num Layers\", \"Test RMSE\", \"Test MAPE\"]\n",
    "\n",
    "for ticker, models in all_best_models.items():\n",
    "    for idx, model_info in enumerate(models, start=1):\n",
    "        table.add_row([\n",
    "            ticker,\n",
    "            idx,\n",
    "            model_info[\"look_back\"],\n",
    "            model_info[\"learning_rate\"],\n",
    "            model_info[\"hidden_size\"],\n",
    "            model_info[\"num_layers\"],\n",
    "            round(model_info[\"test_rmse\"], 4),\n",
    "            round(model_info[\"test_mape\"], 4),\n",
    "        ])\n",
    "\n",
    "# Print the combined table\n",
    "print(\"Top Performing Models:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights for GOOG saved to GOOG_model.pkl\n",
      "Model weights for MSFT saved to MSFT_model.pkl\n",
      "Model weights for META saved to META_model.pkl\n",
      "Model weights for AMZN saved to AMZN_model.pkl\n",
      "Model weights for NFLX saved to NFLX_model.pkl\n",
      "Model weights for TSLA saved to TSLA_model.pkl\n",
      "Model weights for NVDA saved to NVDA_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save model weights\n",
    "for ticker, models in all_best_models.items():\n",
    "    for idx, model_info in enumerate(models, start=1):\n",
    "        # Save the model weights to a pickle file\n",
    "        model_filename = f\"{ticker}_model.pkl\"\n",
    "        with open(\"Saved Models/\"+model_filename, 'wb') as file:\n",
    "            pickle.dump(model_info[\"model\"].state_dict(), file)\n",
    "        print(f\"Model weights for {ticker} saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[User Call](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "+------------+--------------+------------------+\n",
      "|    Date    | Actual Price | Predicted Price  |\n",
      "+------------+--------------+------------------+\n",
      "| 2023-11-20 |    135.43    |      136.71      |\n",
      "| 2023-11-21 |    136.38    |      137.44      |\n",
      "| 2023-11-22 |    138.7     |      137.85      |\n",
      "| 2023-11-24 |    136.94    |      138.32      |\n",
      "| ---------- | ------------ | ---------------- |\n",
      "| 2023-11-27 |     TBA      |      138.57      |\n",
      "+------------+--------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Take user input for choosen_stock\n",
    "# choosen_stock = input(\"Enter the stock symbol ('GOOG', 'MSFT', 'META', 'AMZN', 'NFLX', 'TSLA', 'NVDA'): \")\n",
    "choosen_stock = 'GOOG'\n",
    "\n",
    "# Predict\n",
    "actual_prices, predicted_prices, formatted_dates = predict(choosen_stock)\n",
    "\n",
    "# Display\n",
    "# Remove the first value and shift up the remaining values\n",
    "actual_prices = actual_prices[1:] + ['']\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Date\", \"Actual Price\", \"Predicted Price\"]\n",
    "for date, actual_price, predicted_price in zip(formatted_dates, actual_prices, predicted_prices):\n",
    "    # Add a separator line before the last row\n",
    "    if date == formatted_dates[-1]:\n",
    "        table.add_row([\"-\" * 10, \"-\" * 12, \"-\" * 16])\n",
    "    table.add_row([date, round(float(actual_price), 2) if actual_price != '' else 'TBA', round(float(predicted_price), 2)])\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
